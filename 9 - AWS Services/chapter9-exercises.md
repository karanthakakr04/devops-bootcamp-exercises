# TASK BREAKDOWN

## Exercise 1

> [!CAUTION]
> **The IAM user created in this exercise should only have the necessary permissions to perform their required tasks. Avoid using overly permissive policies such as `AdministratorAccess`.**

- [ ] **Task 1: Login to AWS Management Console**
  - Open your web browser.
  - Navigate to the [AWS Management Console](https://aws.amazon.com/console/).
  - Click on the _Sign in to the Console_ button.
  - Enter your admin username and password.
  - Click on _Sign In_.

- [ ] **Task 2: Navigate to IAM Service**
  - On the AWS Management Console home page, you can either:
    - Find _IAM_ under the _Security, Identity, & Compliance_ section and click on it, or
    - Use the search bar at the top to search for _IAM_ and click on the _IAM_ result.
  - In the IAM dashboard, click on _Users_ in the left sidebar.
  - Click on the _Create user_ button.

- [ ] **Task 3: Enter user details**
  - On the _Specify user details_ page:
    - You will be presented with two options:
      - _AWS IAM Identity Center_
      - _IAM User_
    - Select _IAM User_ to create a new IAM user.
    - _User name:_ Enter a desired username for the IAM user (e.g., "dev-user"). Replace "dev-user" with your preferred username.
    - Check the _Provide user access to the AWS Management Console_ option.
    - Under _Are you providing console access to a person?_, select _I want to create an IAM user_.
    - _Console password:_ Select "Autogenerated password".
    - _Password reset required:_ Keep the _User must create a new password at next sign-in_ option selected.
    - Click on _Next_ button.

> [!NOTE]
> **AWS recommends using AWS IAM Identity Center for centralized access management and short-term credentials. However, for this exercise, we will use _IAM User_ as it is within the scope of our demonstration. For more information, refer to the [AWS IAM Identity Center User Guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_identity-management.html#intro-identity-users). You can also watch this [YouTube tutorial](https://www.youtube.com/watch?v=_KhrGFV_Npw) for a detailed setup of AWS IAM Identity Center.**

- [ ] **Task 4: Assign permissions to the IAM user**
  - On the _Set permissions_ page, you will see three options to set permissions:
    - _Add user to group_
    - _Copy permissions from existing user_
    - _Attach policies directly_
  - AWS recommends adding the user to a group and then attaching policies to the group. Select _Add user to group_.
  - Click on _Create group_.
    - _Group name:_ Enter `devops`.
    - In the _Filter policies_ box, type relevant policies based on the permissions the user needs. Avoid using overly permissive policies such as `AdministratorAccess`.
    - Consider policies like:
      - _AmazonEC2FullAccess_ - For managing EC2 instances.
      - _AmazonVPCFullAccess_ - For managing VPCs, subnets, and security groups.
      - _CloudWatchFullAccess_ - For logging and monitoring.
      - _AmazonS3ReadOnlyAccess_ - For read-only access to S3 buckets (if needed for storing/retrieving objects).
    - Click the _Create group_ button.
  - Ensure the new user is added to the `devops` group.
  - Click on _Next_ button.

- [ ] **Task 5: Finalize and create the IAM user**
  - On the _Review and create_ page:
    - Review the user details and permissions.
    - Optionally, add tags to the IAM user for better organization and management. Here are some suggested tags:
      - Project: DevOpsBootcamp
      - Role: NodeJSAppDeployment
      - Environment: Development
      - Department: Engineering
      - CreatedBy: Admin
      - CreationDate: 2024-05-19
    - If everything looks correct, click on the _Create user_ button.

- [ ] **Task 6: Save IAM user credentials**
  - You will be presented with the _Step 4: Retrieve password_ page.
  - Click on the _Download .csv_ button to download the credentials file, which contains the access key ID, secret access key, and password for the IAM user.
  - Store the downloaded file securely, as it contains sensitive information.
  - Click on the _Return to users list_ button.

> [!WARNING]
> **Make sure to store the downloaded .csv file containing the IAM user credentials securely. Do not share or expose the credentials publicly, as they provide access to your AWS resources.**

## Exercise 2

- [ ] **Task 1: Install AWS CLI**
  - Go to the [AWS CLI installation guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), choose the appropriate installation method for your operating system, and follow the provided installation instructions.
  - Verify the installation by running:

    ```bash
    aws --version
    ```

- [ ] **Task 2: Configure AWS CLI with IAM user credentials**
  - Run the following command to configure AWS CLI:

    ```bash
    aws configure
    ```

  - Provide the following information when prompted:
    - AWS Access Key ID: Enter the access key ID obtained from Exercise 1.
    - AWS Secret Access Key: Enter the secret access key obtained from Exercise 1.
    - Default region name: Enter the desired AWS region (e.g., us-east-1).
    - Default output format: Press Enter to use the default format (json).

- [ ] **Task 3: Verify AWS CLI configuration**
  - Run the following command to verify the configured credentials:

    ```bash
    aws sts get-caller-identity
    ```

  - Run the following command to verify the configured region and output format:

    ```bash
    aws configure list
    ```

## Exercise 3

- [ ] **Task 1: Create a new VPC**
  - Run the following command to create a new VPC:

    ```bash
    aws ec2 create-vpc --cidr-block 10.0.0.0/16 --tag-specifications 'ResourceType=vpc,Tags=[{Key=Name,Value=my-vpc}]' --query Vpc.VpcId --output text
    ```

  - Take note of the VPC ID returned by the command.

- [ ] **Task 2: Create a subnet within the VPC**
  - Run the following command to create a subnet within the VPC:

    ```bash
    aws ec2 create-subnet --vpc-id <VPC_ID> --cidr-block 10.0.1.0/24 --availability-zone <AVAILABILITY_ZONE> --tag-specifications 'ResourceType=subnet,Tags=[{Key=Name,Value=my-subnet}]' --query Subnet.SubnetId --output text
    ```

  - Replace `<vpc-id>` with the VPC ID obtained in Task 1.
  - Take note of the Subnet ID returned by the command.

- [ ] **Task 3: Create a security group**
  - Run the following command to create a security group:

    ```bash
    aws ec2 create-security-group --group-name my-sg --description "My security group" --vpc-id <VPC_ID> --tag-specifications 'ResourceType=security-group,Tags=[{Key=Name,Value=my-sg}]' --query GroupId --output text
    ```

  - Replace `<vpc-id>` with the VPC ID obtained in Task 1.
  - Take note of the Security Group ID returned by the command.

- [ ] **Task 4: Configure Security Group Rules**
  - Run the following command to allow inbound SSH (port 22) access from a specific IP address or range:
  
    ```bash
    aws ec2 authorize-security-group-ingress --group-id <SECURITY_GROUP_ID> --protocol tcp --port 22 --cidr <YOUR_IP_ADDRESS>/32
    ```

  - Replace `<SECURITY_GROUP_ID>` with the Security Group ID from Task 3.
  - Replace `<YOUR_IP_ADDRESS>` with your specific IP address or the IP range you want to allow SSH access from. For example, if your IP address is `203.0.113.0`, you would use `203.0.113.0/32`. If you want to allow access from a range, you can use a subnet mask like `203.0.113.0/24`.
  - Run the following command to allow inbound HTTP (port 80) access:

    ```bash
    aws ec2 authorize-security-group-ingress --group-id <SECURITY_GROUP_ID> --protocol tcp --port 80 --cidr 0.0.0.0/0
    ```

## Exercise 4

- [ ] **Task 1: Create Key Pair**
  - Run the following command to create a new key pair:

    ```bash
    aws ec2 create-key-pair --key-name my-key --query 'KeyMaterial' --output text > my-key.pem
    ```

  - This command creates a new key pair named "my-key" and saves the private key to a file named "my-key.pem" in the current directory.
  - For Linux and MacOS, secure the key pair file by running the following command:

    ```bash
    chmod 400 my-key.pem
    ```

- [ ] **Task 2: Get the Latest Amazon Linux 2 AMI ID**
  - Run the following command to retrieve the latest Amazon Linux 2 AMI ID:

    ```bash
    aws ssm get-parameters --names /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 --region <REGION> --query 'Parameters[0].[Value]' --output text
    ```

  - Replace `<REGION>` with your desired AWS region (e.g., us-east-1).
  - Save the AMI ID returned by the command.

- [ ] **Task 3: Create EC2 Instance**
  - Run the following command to create an EC2 instance:

    ```bash
    aws ec2 run-instances --image-id <AMI_ID> --instance-type t2.micro --key-name my-key --security-group-ids <SECURITY_GROUP_ID> --subnet-id <SUBNET_ID> --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=my-instance}]' --block-device-mappings '[{"DeviceName":"/dev/xvda","Ebs":{"VolumeSize":8,"VolumeType":"gp3","DeleteOnTermination":true,"Encrypted":true}}]' --metadata-options '{"HttpTokens":"required","HttpPutResponseHopLimit":1,"HttpEndpoint":"enabled"}' --query 'Instances[0].InstanceId' --output text
    ```

  - Replace `<AMI_ID>` with the AMI ID from Task 2.
  - Replace `<SECURITY_GROUP_ID>` with the Security Group ID from Exercise 3, Task 3.
  - Replace `<SUBNET_ID>` with the Subnet ID from Exercise 3, Task 2.
  - The command uses the `t2.micro` instance type, which is eligible for the free tier.
  - The `--block-device-mappings` option specifies the EBS volume configuration, including the volume size (8 GB), volume type (gp2), and encryption.
  - The `--metadata-options` option configures the instance metadata service to require token-based access and limit the number of hops.
  - Save the Instance ID returned by the command.

- [ ] **Task 4: Verify EC2 Instance Creation**
  - Run the following command to describe the EC2 instance:

    ```bash
    aws ec2 describe-instances --instance-ids <INSTANCE_ID>
    ```

  - Replace `<INSTANCE_ID>` with the Instance ID from Task 3.
  - Review the output to ensure the instance details are correct, such as the VPC, subnet, security group, and key pair.

- [ ] **Task 5: Retrieve Public IP Address**
  - Run the following command to retrieve the public IP address of the EC2 instance:

    ```bash
    aws ec2 describe-instances --instance-ids <INSTANCE_ID> --query 'Reservations[0].Instances[0].PublicIpAddress' --output text
    ```

  - Replace `<INSTANCE_ID>` with the Instance ID from Task 3.
  - Save the public IP address for connecting to the instance via SSH in Exercise 5.

This refined task list incorporates the following best practices:

- Using the latest Amazon Linux 2 AMI ID retrieved via AWS Systems Manager Parameter Store.
- Configuring the EBS volume with encryption and appropriate size and type.
- Configuring the instance metadata service to require token-based access and limit the number of hops.
- Using the `t2.micro` instance type, which is eligible for the free tier.

## Exercise 5

> [!IMPORTANT]
> **The following task list assumes you are using an Amazon Linux 2023 AMI for your EC2 instance. If you are using a different operating system, such as Ubuntu or another Linux distribution, please be aware that the specific commands and steps may vary. In such cases, it is recommended to refer to the official documentation and guides specific to your operating system for installing Docker and Docker Compose.**

- [ ] **Task 1: SSH into the EC2 instance**
  - Open a terminal or command prompt.
  - Navigate to the directory where you saved the key pair file (`my-key.pem`) in Exercise 4.
  - Run the following command to SSH into the EC2 instance:

    ```bash
    ssh -i /path/to/my-key.pem ec2-user@<PUBLIC_IP_ADDRESS>
    ```

  - Replace `/path/to/my-key.pem` with the actual path to your key pair file.
  - Replace `<PUBLIC_IP_ADDRESS>` with the public IP address of the EC2 instance obtained in Exercise 4, Task 5.
  - If prompted, type "yes" to add the instance to the known hosts list.

- [ ] **Task 2: Update the installed packages and package cache on your instance**
  - Run the following command to update the installed packages and package cache:

    ```bash
    sudo yum update -y
    ```

- [ ] **Task 3: Install Docker**
  - Run the following command to install the most recent Docker Community Edition package:

    ```bash
    sudo amazon-linux-extras install docker -y
    ```

  - This command installs Docker using the Amazon Linux Extras repository.
  - For more detailed instructions and alternative installation methods, refer to the [official AWS documentation on installing Docker](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-docker.html).

- [ ] **Task 4: Start the Docker service**
  - Run the following command to start the Docker service:

    ```bash
    sudo service docker start
    ```

- [ ] **Task 5: Add the `ec2-user` to the `docker` group**
  - Run the following command to add the `ec2-user` to the `docker` group:

    ```bash
    sudo usermod -aG docker ec2-user
    ```

  - This command grants the `ec2-user` permission to run Docker commands without using `sudo`.

- [ ] **Task 6: Log out and log back in to pick up the new `docker` group permissions**
  - Log out of the SSH session by running:

    ```bash
    exit
    ```

  - SSH back into the EC2 instance using the same command from Task 1.
  - This ensures that the new SSH session has the appropriate `docker` group permissions.

- [ ] **Task 7: Verify Docker installation and permissions**
  - Run the following command to verify that the `ec2-user` can run Docker commands without using `sudo`:

    ```bash
    docker ps
    ```

  - If Docker is installed correctly and the `ec2-user` has the necessary permissions, you should see an empty list of containers (since no containers are running yet).

- [ ] **Task 8: Install Docker Compose**
  - Since Amazon Linux 2023 AMI does not come with Docker Compose pre-installed, we need to install it separately.
  - Run the following commands to download and install the latest version of Docker Compose:

    ```bash
    sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
    sudo chmod +x /usr/local/bin/docker-compose
    ```

  - Verify the installation by running:

    ```bash
    docker-compose version
    ```

  - You should see the version information for Docker Compose.

> [!TIP]
> For detailed installation guides and additional information, you can refer to the following resources:
>
> - [How To configure Docker & Docker-Compose in AWS EC2 [Amazon Linux 2023 AMI]](https://medium.com/@fredmanre/how-to-configure-docker-docker-compose-in-aws-ec2-amazon-linux-2023-ami-ab4d10b2bcdc)
> - [Docker & Docker Compose Installation Guide for Amazon Linux 2023](https://gist.github.com/npearce/6f3c7826c7499587f00957fee62f8ee9)

## Exercise 6

- [ ] **Task 1: Use `docker init` to generate Docker configuration files**
  - Open a terminal and navigate to the `aws-exercises/app` directory.
  - Run the `docker init` command to automatically generate the following files:
    - `.dockerignore`
    - `Dockerfile`
    - `compose.yaml`
  - Provide the necessary information when prompted, such as the application platform, desired Node.js version, package manager, entry point, and port.

- [ ] **Task 2: Review and customize the generated Docker configuration files**
  - Open the generated `Dockerfile` and review its contents.
  - Customize the `Dockerfile` if needed, based on your application's specific requirements.
  - Open the generated `compose.yaml` file and review its contents.
  - Customize the `compose.yaml` file if needed, such as adding additional services or volumes.
  - Open the generated `.dockerignore` file and review its contents.
  - Customize the `.dockerignore` file if needed, adding any additional files or directories to be excluded from the Docker build context.

- [ ] **Task 3: Commit and push the changes**
  - Open a terminal and navigate to the root directory of your project.
  - Run the following commands to stage and commit the `.dockerignore` file:

    ```bash
    git add .dockerignore
    git commit -m "Add .dockerignore file"
    ```

  - Run the following commands to stage and commit the `Dockerfile`:

    ```bash
    git add Dockerfile
    git commit -m "Add Dockerfile for containerization"
    ```

  - Run the following commands to stage and commit the `compose.yaml` file:

    ```bash
    git add compose.yaml
    git commit -m "Add Docker Compose configuration"
    ```

  - Push the changes to your remote repository:

    ```bash
    git push origin main
    ```

## Exercise 7: Add "deploy to EC2" step to your existing pipeline

- [ ] Task 1: Set up SSH credentials in Jenkins
  - In the Jenkins web interface, navigate to "Manage Jenkins" > "Manage Credentials".
  - Click on "Jenkins" in the "Stores scoped to Jenkins" section.
  - Click on "Global credentials" and then "Add Credentials".
  - Select "SSH Username with private key" as the kind.
  - Provide a meaningful ID and description for the credentials.
  - Enter the username for accessing the EC2 instance (e.g., "ubuntu").
  - Select "Enter directly" for the private key and paste the contents of the private key file associated with the EC2 instance.
  - Click "OK" to save the credentials.

- [ ] Task 2: Update the Jenkinsfile
  - Open the Jenkinsfile from the previous exercise's project.
  - Add a new stage called "Deploy to EC2" after the existing stages.
  - Inside the new stage, use the `sshagent` block to connect to the EC2 instance using the SSH credentials created in the previous task.
  - Use the `sh` step to execute the necessary commands to deploy the application on the EC2 instance. For example:

    ```groovy
    stage('Deploy to EC2') {
      steps {
        sshagent(['ec2-ssh-credentials']) {
          sh '''
            ssh -o StrictHostKeyChecking=no ubuntu@<ec2-instance-public-ip> "
              docker stop my-app || true
              docker rm my-app || true
              docker pull <docker-registry>/<image-name>:<tag>
              docker run -d --name my-app -p 80:3000 <docker-registry>/<image-name>:<tag>
            "
          '''
        }
      }
    }
    ```

  - Replace `<ec2-instance-public-ip>`, `<docker-registry>`, `<image-name>`, and `<tag>` with the appropriate values for your EC2 instance and Docker image.

- [ ] Task 3: Test the deployment
  - Push the updated Jenkinsfile to the Git repository.
  - Trigger the Jenkins pipeline and verify that the application is deployed successfully on the EC2 instance.
  - Access the application using the public IP or DNS name of the EC2 instance.

## Exercise 8: Configure access from browser (EC2 Security Group)

- [ ] Task 1: Retrieve the security group ID
  - Open the AWS Management Console and navigate to the EC2 dashboard.
  - In the left sidebar, click on "Instances" to view the list of EC2 instances.
  - Select the EC2 instance where your application is deployed.
  - In the "Description" tab, locate the "Security groups" section and note down the security group ID associated with the instance.

- [ ] Task 2: Modify the security group using AWS CLI
  - Open a terminal or command prompt.
  - Run the following command to add an inbound rule to the security group, allowing access to the application port (e.g., port 80) from any IP address:

    ```bash
    aws ec2 authorize-security-group-ingress --group-id <security-group-id> --protocol tcp --port 80 --cidr 0.0.0.0/0
    ```

  - Replace `<security-group-id>` with the ID of the security group associated with your EC2 instance.
  - This command allows incoming traffic on port 80 from any IP address.

- [ ] Task 3: Verify the updated security group
  - Run the following command to describe the security group and verify the new inbound rule:

    ```bash
    aws ec2 describe-security-groups --group-ids <security-group-id>
    ```

  - Replace `<security-group-id>` with the ID of the security group.
  - Look for the new inbound rule in the output and ensure it allows traffic on port 80 from the specified IP range.

- [ ] Task 4: Access the application from a web browser
  - Open a web browser.
  - Enter the public IPv4 address (e.g., http://54.237.83.125:8080) or public IPv4 DNS name (e.g., http://ec2-54-237-83-125.compute-1.amazonaws.com:8080) of your EC2 instance in the address bar.
  - Verify that you can access the deployed application successfully.

### Best Practices

1. **Principle of Least Privilege**: When configuring security group rules, follow the principle of least privilege. Only allow the minimum required access and ports necessary for your application to function properly. Avoid using overly permissive rules like allowing all traffic from any IP address.

2. **Use Specific IP Ranges**: Instead of allowing access from any IP address (0.0.0.0/0), consider restricting access to specific IP ranges or addresses that require access to your application. This can be achieved by specifying the appropriate CIDR block in the `--cidr` parameter of the `authorize-security-group-ingress` command.

3. **Regularly Review and Audit**: Regularly review and audit your security group configurations to ensure that only the necessary inbound and outbound rules are in place. Remove any unused or unnecessary rules to maintain a secure environment.

4. **Document Security Group Configuration**: Maintain documentation of your security group configurations, including the purpose and justification for each inbound and outbound rule. This helps in understanding the security posture of your application and makes it easier to manage and update the configurations in the future.

5. **Use Naming Conventions**: Follow a consistent naming convention for your security groups to easily identify their purpose and associated resources. This helps in managing and organizing security groups effectively.

6. **Monitor and Alert**: Implement monitoring and alerting mechanisms to detect and notify you of any unauthorized or suspicious changes to your security group configurations. This enables you to take prompt action in case of any security breaches or misconfigurations.

By following these best practices, you can enhance the security of your EC2 instances and ensure that only authorized access is granted to your application.

## Exercise 9: Configure automatic triggering of multi-branch pipeline

- [ ] Task 1: Update the Jenkinsfile with branch-based logic
  - Open the Jenkinsfile in your project repository.
  - Add a condition to execute certain stages only for the master branch. For example:

    ```groovy
    stage('Deploy') {
      when {
        branch 'master'
      }
      steps {
        // Deployment steps for the master branch
      }
    }

    stage('Run Tests') {
      when {
        not {
          branch 'master'
        }
      }
      steps {
        // Test execution steps for non-master branches
      }
    }
    ```

  - Use the `when` directive to specify conditions for executing specific stages based on the branch name.
  - In this example, the "Deploy" stage runs only for the master branch, while the "Run Tests" stage runs for all other branches.

- [ ] Task 2: Set up a multi-branch pipeline in Jenkins
  - In the Jenkins web interface, click on "New Item".
  - Enter a name for your multi-branch pipeline and select "Multibranch Pipeline" as the item type.
  - Click "OK" to create the multi-branch pipeline.
  - In the configuration page, under the "Branch Sources" section, click "Add source" and select "Git".
  - Provide the repository URL and credentials for accessing the Git repository.
  - Configure any additional settings, such as the branch naming convention or automatic branch discovery.
  - Save the multi-branch pipeline configuration.

- [ ] Task 3: Configure webhook in the Git repository
  - Go to your Git repository's settings page (e.g., GitHub, GitLab).
  - Navigate to the webhooks section.
  - Click on "Add webhook" or similar option.
  - Provide the Jenkins URL where the webhook should be sent. For example:

    ```bash
    http://<jenkins-url>/multibranch-webhook-trigger/invoke?token=<token>
    ```

  - Replace `<jenkins-url>` with the URL of your Jenkins server and `<token>` with a unique token for authentication (if required).
  - Select the desired events that should trigger the webhook (e.g., push events).
  - Save the webhook configuration.

- [ ] Task 4: Test the multi-branch pipeline
  - Create a new branch in your Git repository and push some changes.
  - Verify that the multi-branch pipeline automatically detects the new branch and triggers the pipeline execution.
  - Confirm that the appropriate stages are executed based on the branch-based logic defined in the Jenkinsfile.
